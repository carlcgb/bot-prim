# üíæ Options de Stockage Gratuites pour PRIMBOT

Ce guide pr√©sente les meilleures solutions **100% gratuites** pour h√©berger :
- üìö **Base de connaissances** (vectorielle)
- üí¨ **Historique des conversations** (questions/r√©ponses)
- üß† **Donn√©es d'apprentissage** (feedback, am√©liorations)

## üèÜ Recommandation : Architecture Hybride

### Option 1 : Supabase (Recommand√©) ‚≠ê

**Pourquoi Supabase ?**
- ‚úÖ **PostgreSQL gratuit** (500 MB) + **pgvector** (extensions vectorielles)
- ‚úÖ **Storage gratuit** (1 GB) pour fichiers/images
- ‚úÖ **API REST automatique**
- ‚úÖ **Authentification incluse** (si besoin)
- ‚úÖ **Tout dans un seul service**

**Limites gratuites :**
- 500 MB base de donn√©es
- 1 GB stockage fichiers
- 2 GB bande passante/mois
- 50,000 requ√™tes API/mois

**Architecture :**
```
Supabase PostgreSQL
‚îú‚îÄ‚îÄ Table: knowledge_base (pgvector pour embeddings)
‚îú‚îÄ‚îÄ Table: conversations (historique Q/A)
‚îú‚îÄ‚îÄ Table: feedback (apprentissage)
‚îî‚îÄ‚îÄ Storage: images/screenshots
```

**Avantages :**
- Une seule plateforme pour tout
- pgvector = recherche vectorielle native PostgreSQL
- Facile √† migrer depuis ChromaDB
- API REST automatique
- Dashboard web int√©gr√©

**Inconv√©nients :**
- Limite de 500 MB (mais extensible)
- N√©cessite migration depuis ChromaDB

---

### Option 2 : Qdrant Cloud + Supabase

**Qdrant Cloud** (Base vectorielle d√©di√©e)
- ‚úÖ **1 GB gratuit** pour embeddings
- ‚úÖ **Performance optimale** pour recherche vectorielle
- ‚úÖ **API simple**
- ‚úÖ **D√©j√† utilis√© par de grandes entreprises**

**Supabase** (Pour conversations/feedback)
- ‚úÖ PostgreSQL pour donn√©es relationnelles
- ‚úÖ Storage pour fichiers

**Architecture :**
```
Qdrant Cloud ‚Üí Base de connaissances (vectorielle)
Supabase ‚Üí Conversations + Feedback + Images
```

**Avantages :**
- Meilleure performance pour recherche vectorielle
- S√©paration des pr√©occupations
- Scalable

**Inconv√©nients :**
- Deux services √† g√©rer
- Plus complexe √† configurer

---

### Option 3 : MongoDB Atlas + LanceDB

**MongoDB Atlas** (Gratuit)
- ‚úÖ **512 MB gratuit**
- ‚úÖ **Flexible** (NoSQL)
- ‚úÖ **Parfait pour conversations** (documents JSON)

**LanceDB** (Vectorielle)
- ‚úÖ **Serverless gratuit**
- ‚úÖ **Simple √† utiliser**
- ‚úÖ **Compatible avec ChromaDB**

**Avantages :**
- MongoDB excellent pour conversations
- LanceDB gratuit et performant
- Facile √† migrer

**Inconv√©nients :**
- Deux services
- LanceDB moins mature que Qdrant

---

## üìä Comparaison D√©taill√©e

| Solution | Base Vectorielle | Base Conversations | Stockage Fichiers | Limite Gratuite | Difficult√© |
|----------|-----------------|-------------------|------------------|----------------|------------|
| **Supabase** | ‚úÖ pgvector | ‚úÖ PostgreSQL | ‚úÖ 1 GB | 500 MB DB | ‚≠ê‚≠ê Facile |
| **Qdrant + Supabase** | ‚úÖ Qdrant (1 GB) | ‚úÖ Supabase | ‚úÖ Supabase | 1 GB + 500 MB | ‚≠ê‚≠ê‚≠ê Moyen |
| **MongoDB + LanceDB** | ‚úÖ LanceDB | ‚úÖ MongoDB | ‚ö†Ô∏è Limit√© | 512 MB + Serverless | ‚≠ê‚≠ê‚≠ê Moyen |
| **Weaviate Cloud** | ‚úÖ Weaviate | ‚ö†Ô∏è Int√©gr√© | ‚ö†Ô∏è Limit√© | 1 GB | ‚≠ê‚≠ê‚≠ê Difficile |
| **Pinecone** | ‚úÖ Pinecone | ‚ùå Non | ‚ùå Non | 1 projet gratuit | ‚≠ê‚≠ê Facile |

---

## üöÄ Guide d'Impl√©mentation : Supabase (Recommand√©)

### √âtape 1 : Cr√©er un compte Supabase

1. Allez sur [supabase.com](https://supabase.com)
2. Cr√©ez un compte gratuit
3. Cr√©ez un nouveau projet
4. Notez votre URL et API key

### √âtape 2 : Installer les d√©pendances

```bash
pip install supabase pgvector psycopg2-binary
```

### √âtape 3 : Configuration

Cr√©ez `storage_config.py` :

```python
import os
from supabase import create_client, Client
import psycopg2
from psycopg2.extras import execute_values
import json

# Configuration Supabase
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
SUPABASE_DB_URL = os.getenv("SUPABASE_DB_URL")  # Connection string PostgreSQL

# Clients
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
```

### √âtape 4 : Migration depuis ChromaDB

Cr√©ez `migrate_to_supabase.py` :

```python
from knowledge_base import collection, query_knowledge_base
from storage_config import supabase, SUPABASE_DB_URL
import psycopg2
import json

def migrate_knowledge_base():
    """Migre la base ChromaDB vers Supabase avec pgvector."""
    
    # 1. Cr√©er la table avec pgvector
    conn = psycopg2.connect(SUPABASE_DB_URL)
    cur = conn.cursor()
    
    # Activer l'extension pgvector
    cur.execute("CREATE EXTENSION IF NOT EXISTS vector;")
    
    # Cr√©er la table
    cur.execute("""
        CREATE TABLE IF NOT EXISTS knowledge_base (
            id TEXT PRIMARY KEY,
            content TEXT NOT NULL,
            url TEXT,
            title TEXT,
            chunk_index INTEGER,
            images JSONB,
            embedding vector(384)  -- Dimension pour all-MiniLM-L6-v2
        );
        
        CREATE INDEX IF NOT EXISTS knowledge_base_embedding_idx 
        ON knowledge_base USING ivfflat (embedding vector_cosine_ops);
    """)
    
    conn.commit()
    
    # 2. Migrer les donn√©es depuis ChromaDB
    # (R√©cup√©rer tous les documents et les embeddings)
    # Note: ChromaDB ne permet pas d'exporter facilement les embeddings
    # Il faudra les recalculer avec sentence-transformers
    
    print("‚úÖ Migration termin√©e")
    cur.close()
    conn.close()
```

### √âtape 5 : Nouveau module de stockage

Cr√©ez `storage_supabase.py` :

```python
from storage_config import supabase
from sentence_transformers import SentenceTransformer
import json

# Mod√®le d'embedding (m√™me que ChromaDB)
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

def add_documents_to_supabase(pages_data):
    """Ajoute des documents √† Supabase avec pgvector."""
    from storage_config import SUPABASE_DB_URL
    import psycopg2
    
    conn = psycopg2.connect(SUPABASE_DB_URL)
    cur = conn.cursor()
    
    for page in pages_data:
        url = page['url']
        title = page['title']
        content = page['content']
        images = page.get('images', [])
        
        # Chunking (m√™me logique que ChromaDB)
        chunks = chunk_text(content)
        
        for i, chunk in enumerate(chunks):
            if not chunk.strip():
                continue
            
            # G√©n√©rer l'embedding
            embedding = embedding_model.encode(chunk).tolist()
            
            # Ins√©rer dans Supabase
            chunk_id = f"{url}_{i}"
            cur.execute("""
                INSERT INTO knowledge_base 
                (id, content, url, title, chunk_index, images, embedding)
                VALUES (%s, %s, %s, %s, %s, %s, %s::vector)
                ON CONFLICT (id) DO UPDATE SET
                    content = EXCLUDED.content,
                    embedding = EXCLUDED.embedding
            """, (
                chunk_id,
                chunk,
                url,
                title,
                i,
                json.dumps(images),
                str(embedding)
            ))
    
    conn.commit()
    cur.close()
    conn.close()

def query_supabase(query, n_results=10):
    """Recherche dans Supabase avec pgvector."""
    from storage_config import SUPABASE_DB_URL
    import psycopg2
    
    # G√©n√©rer l'embedding de la requ√™te
    query_embedding = embedding_model.encode(query).tolist()
    
    conn = psycopg2.connect(SUPABASE_DB_URL)
    cur = conn.cursor()
    
    # Recherche vectorielle avec cosine similarity
    cur.execute("""
        SELECT 
            id, content, url, title, chunk_index, images,
            1 - (embedding <=> %s::vector) as distance
        FROM knowledge_base
        ORDER BY embedding <=> %s::vector
        LIMIT %s
    """, (str(query_embedding), str(query_embedding), n_results))
    
    results = cur.fetchall()
    cur.close()
    conn.close()
    
    # Formater comme ChromaDB
    return {
        'documents': [[r[1]] for r in results],
        'metadatas': [[{
            'url': r[2],
            'title': r[3],
            'chunk_index': r[4],
            'images': r[5] if r[5] else ''
        }] for r in results],
        'distances': [[r[6]] for r in results],
        'ids': [[r[0]] for r in results]
    }

def save_conversation(user_id, question, answer, metadata=None):
    """Sauvegarde une conversation dans Supabase."""
    supabase.table('conversations').insert({
        'user_id': user_id,
        'question': question,
        'answer': answer,
        'metadata': metadata or {},
        'created_at': 'now()'
    }).execute()

def get_conversation_history(user_id, limit=50):
    """R√©cup√®re l'historique des conversations."""
    response = supabase.table('conversations')\
        .select('*')\
        .eq('user_id', user_id)\
        .order('created_at', desc=True)\
        .limit(limit)\
        .execute()
    
    return response.data

def save_feedback(conversation_id, rating, comment=None):
    """Sauvegarde un feedback pour l'apprentissage."""
    supabase.table('feedback').insert({
        'conversation_id': conversation_id,
        'rating': rating,  # 1-5 √©toiles
        'comment': comment,
        'created_at': 'now()'
    }).execute()
```

### √âtape 6 : Tables Supabase

Ex√©cutez ce SQL dans le SQL Editor de Supabase :

```sql
-- Table pour la base de connaissances (avec pgvector)
CREATE EXTENSION IF NOT EXISTS vector;

CREATE TABLE IF NOT EXISTS knowledge_base (
    id TEXT PRIMARY KEY,
    content TEXT NOT NULL,
    url TEXT,
    title TEXT,
    chunk_index INTEGER,
    images JSONB,
    embedding vector(384)
);

CREATE INDEX knowledge_base_embedding_idx 
ON knowledge_base USING ivfflat (embedding vector_cosine_ops);

-- Table pour les conversations
CREATE TABLE IF NOT EXISTS conversations (
    id BIGSERIAL PRIMARY KEY,
    user_id TEXT,
    question TEXT NOT NULL,
    answer TEXT NOT NULL,
    metadata JSONB,
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX conversations_user_id_idx ON conversations(user_id);
CREATE INDEX conversations_created_at_idx ON conversations(created_at DESC);

-- Table pour le feedback/apprentissage
CREATE TABLE IF NOT EXISTS feedback (
    id BIGSERIAL PRIMARY KEY,
    conversation_id BIGINT REFERENCES conversations(id),
    rating INTEGER CHECK (rating >= 1 AND rating <= 5),
    comment TEXT,
    created_at TIMESTAMP DEFAULT NOW()
);
```

---

## üîÑ Migration Progressive

Vous pouvez migrer progressivement :

1. **Phase 1** : Garder ChromaDB, ajouter Supabase pour conversations
2. **Phase 2** : Migrer la base de connaissances vers Supabase
3. **Phase 3** : Retirer ChromaDB

---

## üìù Variables d'Environnement

Ajoutez dans `.streamlit/secrets.toml` ou variables d'environnement :

```toml
# Supabase
SUPABASE_URL = "https://xxxxx.supabase.co"
SUPABASE_KEY = "your-anon-key"
SUPABASE_DB_URL = "postgresql://postgres:password@db.xxxxx.supabase.co:5432/postgres"

# Optionnel: Garder ChromaDB en fallback
USE_SUPABASE = true
FALLBACK_TO_CHROMADB = true
```

---

## üéØ Recommandation Finale

**Pour PRIMBOT, je recommande Supabase** car :

1. ‚úÖ **Tout-en-un** : Base vectorielle + conversations + storage
2. ‚úÖ **Gratuit et g√©n√©reux** : 500 MB + 1 GB storage
3. ‚úÖ **Facile √† migrer** : Compatible avec votre code actuel
4. ‚úÖ **Scalable** : Facilement extensible si besoin
5. ‚úÖ **Dashboard int√©gr√©** : Visualisation des donn√©es
6. ‚úÖ **API REST automatique** : Pas besoin de backend custom

**Limites :**
- 500 MB suffit pour ~10,000 documents avec embeddings
- Si vous d√©passez, upgrade √† $25/mois pour 8 GB

---

## üìö Ressources

- [Supabase Documentation](https://supabase.com/docs)
- [pgvector Documentation](https://github.com/pgvector/pgvector)
- [Qdrant Cloud](https://cloud.qdrant.io/)
- [MongoDB Atlas](https://www.mongodb.com/cloud/atlas)

---

## üÜò Support

Pour toute question sur la migration, ouvrez une issue sur GitHub.

