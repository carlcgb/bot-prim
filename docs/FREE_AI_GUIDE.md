# üÜì Guide des Options AI Gratuites pour PRIMBOT

PRIMBOT supporte plusieurs options d'IA gratuites pour r√©pondre √† vos besoins de d√©bogage et d'assistance.

## üéØ Options Disponibles

### 1. Google Gemini (Recommand√©) ‚≠ê

**Avantages:**
- ‚úÖ **Plan gratuit g√©n√©reux** : 60 requ√™tes/minute, 1500 requ√™tes/jour
- ‚úÖ **Mod√®les performants** : Gemini 2.5 Flash (rapide) et Gemini 2.5 Pro (puissant)
- ‚úÖ **Fonction calling** : Support natif pour la recherche dans la base de connaissances
- ‚úÖ **Pas de carte de cr√©dit** requise
- ‚úÖ **API stable** et bien document√©e

**Comment obtenir une cl√© API gratuite:**

1. Allez sur [Google AI Studio](https://aistudio.google.com/)
2. Connectez-vous avec votre compte Google
3. Cliquez sur "Get API Key"
4. Cr√©ez un nouveau projet ou s√©lectionnez un projet existant
5. Copiez votre cl√© API (commence par `AIza...`)

**Mod√®les recommand√©s:**
- `gemini-2.5-flash` : **Recommand√©** - Rapide, gratuit, excellent pour la plupart des cas
- `gemini-2.5-pro` : Plus puissant mais peut avoir des limites sur le plan gratuit
- `gemini-2.0-flash` : Alternative stable

**Configuration:**
```bash
# Variable d'environnement
export GEMINI_API_KEY="votre_cle_api"

# Ou dans Streamlit secrets
GEMINI_API_KEY = "votre_cle_api"
```

---

### 2. Ollama (100% Gratuit, Local) üè†

**Avantages:**
- ‚úÖ **100% gratuit** - Aucune cl√© API n√©cessaire
- ‚úÖ **Fonctionne localement** - Vos donn√©es restent sur votre machine
- ‚úÖ **Aucune limite** de requ√™tes
- ‚úÖ **Mod√®les open-source** performants
- ‚úÖ **Pas de connexion internet** requise (apr√®s installation)

**Inconv√©nients:**
- ‚ö†Ô∏è N√©cessite une installation locale
- ‚ö†Ô∏è Requiert des ressources syst√®me (RAM, CPU)
- ‚ö†Ô∏è Premi√®re installation peut prendre du temps

**Installation:**

1. **T√©l√©chargez Ollama:**
   - Windows: [ollama.ai/download](https://ollama.ai/download)
   - macOS: `brew install ollama`
   - Linux: `curl -fsSL https://ollama.ai/install.sh | sh`

2. **Installez un mod√®le:**
   ```bash
   # Mod√®le recommand√© (√©quilibr√©)
   ollama pull llama3.1
   
   # Ou pour plus de puissance (n√©cessite plus de RAM)
   ollama pull llama3.1:70b
   
   # Mod√®le rapide et l√©ger
   ollama pull llama3.2
   ```

3. **Lancez Ollama:**
   ```bash
   ollama serve
   ```

4. **Configurez PRIMBOT:**
   - Dans l'interface Streamlit, s√©lectionnez "Local (Ollama/LocalAI)"
   - Base URL: `http://localhost:11434/v1`
   - Model: `llama3.1` (ou le mod√®le que vous avez install√©)

**Mod√®les recommand√©s:**

| Mod√®le | Taille | RAM requise | Performance | Usage |
|--------|--------|--------------|-------------|-------|
| `llama3.2` | 2B | ~2 GB | Rapide | Tests rapides |
| `llama3.1` | 8B | ~8 GB | √âquilibr√© | **Recommand√©** |
| `llama3.1:70b` | 70B | ~40 GB | Excellent | Production |
| `mistral` | 7B | ~7 GB | Rapide | Alternative |
| `mixtral` | 8x7B | ~26 GB | Tr√®s puissant | Cas complexes |

**V√©rification:**
```bash
# V√©rifier que Ollama fonctionne
curl http://localhost:11434/api/tags

# Tester un mod√®le
ollama run llama3.1 "Bonjour, comment √ßa va?"
```

---

## üìä Comparaison des Options

| Crit√®re | Gemini | Ollama |
|---------|--------|--------|
| **Co√ªt** | Gratuit (limites) | 100% Gratuit |
| **Installation** | Aucune | Requise |
| **Cl√© API** | Oui (gratuite) | Non |
| **Internet** | Requis | Optionnel |
| **Vitesse** | Tr√®s rapide | D√©pend du hardware |
| **Qualit√©** | Excellente | Bonne √† excellente |
| **Fonction calling** | Natif | Support√© |
| **Limites** | 60 req/min | Aucune |
| **Confidentialit√©** | Donn√©es envoy√©es | 100% local |

---

## üéØ Quelle Option Choisir?

### Choisissez **Gemini** si:
- ‚úÖ Vous voulez une solution **rapide et simple**
- ‚úÖ Vous avez une **connexion internet stable**
- ‚úÖ Vous √™tes **d√©butant** ou pr√©f√©rez ne pas installer de logiciel
- ‚úÖ Vous avez besoin de **r√©ponses tr√®s rapides**
- ‚úÖ Vous travaillez sur des **donn√©es non sensibles**

### Choisissez **Ollama** si:
- ‚úÖ Vous voulez une solution **100% gratuite sans limites**
- ‚úÖ Vous avez des **donn√©es sensibles** (tout reste local)
- ‚úÖ Vous avez un **bon ordinateur** (8GB+ RAM recommand√©)
- ‚úÖ Vous voulez **contr√¥ler totalement** votre environnement
- ‚úÖ Vous n'avez pas toujours **internet** disponible

---

## üöÄ Configuration Optimale

### Pour le D√©veloppement Local

**Option 1: Gemini (Recommand√© pour d√©buter)**
```bash
export GEMINI_API_KEY="votre_cle"
streamlit run app.py
```

**Option 2: Ollama (Pour la confidentialit√©)**
```bash
# Terminal 1: Lancer Ollama
ollama serve

# Terminal 2: Lancer PRIMBOT
streamlit run app.py
# S√©lectionner "Local (Ollama/LocalAI)" dans l'interface
```

### Pour le D√©ploiement (Streamlit Cloud)

**Gemini uniquement** (Ollama n√©cessite un serveur local):
```toml
# Dans Streamlit Cloud secrets
GEMINI_API_KEY = "votre_cle_api"
```

---

## üí° Conseils d'Optimisation

### Pour Gemini:
1. Utilisez `gemini-2.5-flash` par d√©faut (rapide et gratuit)
2. R√©servez `gemini-2.5-pro` pour les questions complexes
3. Respectez les limites (60 req/min) pour √©viter les erreurs

### Pour Ollama:
1. Commencez avec `llama3.1` (8B) - bon √©quilibre
2. Si vous avez 16GB+ RAM, essayez `llama3.1:70b` pour de meilleures r√©ponses
3. Fermez les autres applications pour lib√©rer de la RAM
4. Utilisez `llama3.2` si vous avez moins de 8GB RAM

---

## üîß D√©pannage

### Gemini ne fonctionne pas?
- ‚úÖ V√©rifiez que votre cl√© API est correcte
- ‚úÖ V√©rifiez que vous n'avez pas d√©pass√© les limites gratuites
- ‚úÖ Essayez un autre mod√®le (gemini-2.0-flash)

### Ollama ne fonctionne pas?
- ‚úÖ V√©rifiez que `ollama serve` est lanc√©
- ‚úÖ V√©rifiez que le mod√®le est install√©: `ollama list`
- ‚úÖ Testez l'API: `curl http://localhost:11434/api/tags`
- ‚úÖ V√©rifiez que le port 11434 n'est pas bloqu√© par un firewall

---

## üìö Ressources

- **Gemini**: [Google AI Studio](https://aistudio.google.com/)
- **Ollama**: [ollama.ai](https://ollama.ai/)
- **Documentation Ollama**: [github.com/ollama/ollama](https://github.com/ollama/ollama)

---

**Derni√®re mise √† jour**: D√©cembre 2024

