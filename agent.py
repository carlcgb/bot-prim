import warnings
import os
import logging

# Suppress warnings early, before other imports
os.environ['GRPC_VERBOSITY'] = 'ERROR'
os.environ['GLOG_minloglevel'] = '2'  # Suppress Google logging
warnings.filterwarnings('ignore', category=RuntimeWarning, message='.*duckduckgo_search.*')
warnings.filterwarnings('ignore', category=RuntimeWarning, message='.*ALTS.*')
warnings.filterwarnings('ignore', message='.*ALTS.*')

from knowledge_base import query_knowledge_base
import json

import google.generativeai as genai
from google.protobuf.json_format import MessageToDict

logger = logging.getLogger(__name__)

class PrimAgent:
    def __init__(self, api_key, base_url=None, model="gemini-2.5-flash", provider="Google Gemini"):
        self.provider = provider
        self.model_name = model

        if self.provider == "Google Gemini":
            # Configure Gemini API
            genai.configure(api_key=api_key)
            
            # Define Gemini Tools using FunctionDeclaration - only knowledge base search
            self.gemini_tools = [
                genai.protos.Tool(
                    function_declarations=[
                        genai.protos.FunctionDeclaration(
                            name="search_knowledge_base",
                            description="""Recherche approfondie dans la base de connaissances de la documentation PrimLogix.
                            
UTILISE CET OUTIL pour:
- Trouver des solutions √† des probl√®mes techniques ou erreurs
- Comprendre comment utiliser une fonctionnalit√© sp√©cifique
- Obtenir des proc√©dures d√©taill√©es √©tape par √©tape
- Trouver des exemples de configuration ou d'utilisation
- Rechercher des informations sur des champs, param√®tres, ou options sp√©cifiques

IMPORTANT:
- Utilise des termes techniques pr√©cis dans ta requ√™te (noms de champs, codes d'erreur, noms de fonctionnalit√©s)
- Si la premi√®re recherche ne donne pas de r√©sultats satisfaisants, essaie des variantes de la requ√™te
- Combine les informations de plusieurs r√©sultats pour donner une r√©ponse compl√®te
- Cite toujours les sources (URLs) dans ta r√©ponse finale
- Les r√©sultats incluent des scores de pertinence et des liens directs vers la documentation""",
                            parameters=genai.protos.Schema(
                                type=genai.protos.Type.OBJECT,
                                properties={
                                    "query": genai.protos.Schema(
                                        type=genai.protos.Type.STRING,
                                        description="Requ√™te de recherche d√©taill√©e et sp√©cifique. Utilise des termes techniques pr√©cis, codes d'erreur, noms de fonctionnalit√©s, ou descriptions de probl√®mes. Exemples: 'erreur E001', 'configuration export CSV', 'champ date de facturation', 'proc√©dure cr√©ation client'."
                                    )
                                },
                                required=["query"]
                            )
                        )
                    ]
                )
            ]
            
            # Map for execution
            self.tool_map = {
                "search_knowledge_base": self._search_kb
            }

        elif self.provider == "Local (Ollama/LocalAI)":
            # Configure for Ollama/LocalAI (OpenAI-compatible API)
            self.base_url = base_url or "http://localhost:11434/v1"
            self.api_key = api_key or "ollama"  # Ollama doesn't require a real key
            self.tool_map = {
                "search_knowledge_base": self._search_kb
            }
            # Ollama will use OpenAI-compatible client
            try:
                from openai import OpenAI
                self.ollama_client = OpenAI(
                    base_url=self.base_url,
                    api_key=self.api_key
                )
            except ImportError:
                logger.warning("openai package not installed. Install with: pip install openai")
                self.ollama_client = None
        else:
            raise ValueError(f"Provider '{self.provider}' not supported. Use 'Google Gemini' or 'Local (Ollama/LocalAI)'.")


    def _search_kb(self, query):
        logger.debug(f"Searching KB for: {query}")
        try:
            # First check if knowledge base is accessible
            from knowledge_base import collection
            kb_count = collection.count()
            
            if kb_count == 0:
                return """‚ö†Ô∏è **Base de connaissances vide**

La base de connaissances PrimLogix n'a pas encore √©t√© initialis√©e.

**Solutions:**
1. Dans l'interface Streamlit, utilisez le bouton "Initialiser la base de connaissances"
2. Ou ex√©cutez manuellement: `python ingest.py`
3. V√©rifiez que le dossier `chroma_db/` existe et contient des donn√©es

Une fois initialis√©e, je pourrai rechercher dans la documentation pour vous aider."""
            
            # Search with optimized number of results for better performance
            # Reduced from 10 to 6 for faster responses while maintaining quality
            results = query_knowledge_base(query, n_results=6)
            
            # Check if results are valid
            if not results:
                return f"‚ùå Erreur: Aucun r√©sultat retourn√© par la base de connaissances pour la requ√™te: '{query}'"
            
            if not results.get('documents') or not results['documents']:
                return f"‚ùå Aucune documentation trouv√©e pour la requ√™te: '{query}'\n\n**Suggestions:**\n- Essayez des termes plus g√©n√©raux\n- V√©rifiez l'orthographe\n- Utilisez des mots-cl√©s techniques de PrimLogix"
            
            if not results['documents'][0]:
                 return f"‚ùå Aucune documentation pertinente trouv√©e dans la base de connaissances PrimLogix pour cette requ√™te: '{query}'.\n\n**Base de connaissances:** {kb_count} documents disponibles.\n\n**Suggestions:**\n- Reformulez votre question avec des termes techniques\n- Essayez des mots-cl√©s sp√©cifiques √† PrimLogix"
            
            docs = results['documents'][0]
            metadatas = results['metadatas'][0]
            distances = results.get('distances', [None])[0] if results.get('distances') else [None] * len(docs)
            
            # Filter results by relevance score to improve quality and speed
            # Only include results with relevance >= 40% for better context
            filtered_docs = []
            filtered_metadatas = []
            filtered_distances = []
            
            for i, doc in enumerate(docs):
                if not doc or not doc.strip():
                    continue
                
                # Calculate relevance score
                relevance_score = None
                if distances and i < len(distances) and distances[i] is not None:
                    # Lower distance = more relevant, convert to percentage-like score
                    relevance_score = max(0, min(100, int((1 - distances[i]) * 100)))
                    
                    # Filter: only include results with relevance >= 40%
                    if relevance_score < 40:
                        continue  # Skip low-relevance results
                
                filtered_docs.append(doc)
                filtered_metadatas.append(metadatas[i] if i < len(metadatas) else {})
                filtered_distances.append(distances[i] if distances and i < len(distances) else None)
            
            # Use filtered results
            docs = filtered_docs
            metadatas = filtered_metadatas
            distances = filtered_distances
            
            if not docs:
                return f"‚ùå Aucune documentation pertinente trouv√©e (score de pertinence < 40%) pour la requ√™te: '{query}'.\n\n**Suggestions:**\n- Reformulez votre question avec des termes techniques\n- Essayez des mots-cl√©s sp√©cifiques √† PrimLogix"
            
            # Build detailed context with relevance scores (optimized)
            context_parts = []
            seen_urls = set()  # Track unique URLs to avoid duplicates
            
            context_parts.append(f"üìö **R√©sultats de recherche** (requ√™te: \"{query}\")\n")
            context_parts.append(f"Trouv√© {len(docs)} document(s) pertinent(s) dans la base de connaissances PrimLogix:\n")
            
            for i, doc in enumerate(docs):
                source = metadatas[i].get('url', 'URL inconnue') if i < len(metadatas) else 'URL inconnue'
                title = metadatas[i].get('title', 'Sans titre') if i < len(metadatas) else 'Sans titre'
                chunk_idx = metadatas[i].get('chunk_index', '?') if i < len(metadatas) else '?'
                
                # Calculate relevance score for display
                relevance_score = None
                relevance_badge = ""
                if distances and i < len(distances) and distances[i] is not None:
                    # Lower distance = more relevant, convert to percentage-like score
                    relevance_score = max(0, min(100, int((1 - distances[i]) * 100)))
                    if relevance_score >= 80:
                        relevance_badge = "üü¢ [Tr√®s pertinent]"
                    elif relevance_score >= 60:
                        relevance_badge = "üü° [Pertinent]"
                    elif relevance_score >= 40:
                        relevance_badge = "üü† [Mod√©r√©ment pertinent]"
                    relevance_badge += f" (Score: {relevance_score}%)"
                
                # Limit document length to avoid token limits
                doc_content = doc[:8000] if len(doc) > 8000 else doc
                
                # Build detailed source info with clickable link
                # Limit document content to avoid token limits (max 8000 chars per document)
                doc_content = doc[:8000] if len(doc) > 8000 else doc
                if len(doc) > 8000:
                    doc_content += "\n\n[... contenu tronqu√© pour optimiser la r√©ponse ...]"
                
                source_info = f"\n### üìÑ Document #{i+1}: {title}"
                if relevance_badge:
                    source_info += f" {relevance_badge}"
                source_info += f"\n**üîó Lien direct:** [{title}]({source})"
                source_info += f"\n**URL:** {source}"
                source_info += f"\n**Chunk:** {chunk_idx}"
                source_info += f"\n\n**Contenu:**\n{doc_content}\n"
                source_info += "\n" + "‚îÄ" * 60 + "\n"
                
                context_parts.append(source_info)
            
            if not context_parts:
                return "‚ùå Aucune documentation pertinente trouv√©e dans la base de connaissances PrimLogix."
            
            # Combine all context
            response_text = "\n".join(context_parts)
            
            # Add summary statistics with links to sources
            response_text += f"\n\n**üìä R√©sum√©:** {len(docs)} document(s) trouv√©(s)"
            
            # Add direct links section for easy access
            response_text += "\n\n**üîó Liens directs vers la documentation:**\n"
            seen_source_urls = set()
            for i, doc in enumerate(docs):
                if i < len(metadatas):
                    source = metadatas[i].get('url', '')
                    title = metadatas[i].get('title', 'Sans titre')
                    if source and source not in seen_source_urls:
                        seen_source_urls.add(source)
                        response_text += f"- [{title}]({source})\n"
            
            return response_text
        except ImportError as e:
            logger.error(f"Import error in KB search: {e}", exc_info=True)
            return f"""‚ùå **Erreur d'importation**

Impossible d'importer le module de base de connaissances.

**Solution:** V√©rifiez que tous les modules sont install√©s:
```bash
pip install -r requirements.txt
```"""
        except AttributeError as e:
            logger.error(f"Attribute error in KB search: {e}", exc_info=True)
            return f"""‚ùå **Erreur de configuration de la base de connaissances**

La base de connaissances n'est pas correctement configur√©e.

**Solutions:**
1. R√©initialisez la base: `python ingest.py`
2. V√©rifiez que le dossier `chroma_db/` existe
3. V√©rifiez les permissions d'acc√®s au dossier"""
        except Exception as e:
            logger.error(f"Error searching KB: {e}", exc_info=True)
            error_type = type(e).__name__
            error_msg = str(e)
            
            # Provide helpful error messages based on error type
            if "collection" in error_msg.lower() or "chromadb" in error_msg.lower():
                return f"""‚ùå **Erreur d'acc√®s √† la base de connaissances**

**Erreur:** {error_type}: {error_msg}

**Solutions:**
1. V√©rifiez que ChromaDB est install√©: `pip install chromadb`
2. R√©initialisez la base: `python ingest.py`
3. V√©rifiez que le dossier `chroma_db/` n'est pas corrompu
4. Si le probl√®me persiste, supprimez `chroma_db/` et r√©initialisez"""
            else:
                return f"""‚ùå **Erreur lors de la recherche**

**Erreur:** {error_type}: {error_msg}

**Requ√™te:** {query}

**Solutions:**
1. R√©essayez avec une requ√™te diff√©rente
2. V√©rifiez que la base de connaissances est initialis√©e
3. Consultez les logs pour plus de d√©tails"""


    def run(self, messages):
        if self.provider == "Google Gemini":
            return self._run_gemini(messages)
        elif self.provider == "Local (Ollama/LocalAI)":
            return self._run_ollama(messages)
        else:
            raise ValueError(f"Provider '{self.provider}' not supported.")

    def _run_gemini(self, messages):
        # Convert OpenAI messages to Gemini History
        history = []
        for msg in messages[:-1]:  # All but last
            role = msg.get('role')
            content = msg.get('content', '')
            
            # Skip tool messages in history for now (Gemini handles this differently)
            if role == 'tool':
                continue
                
            if role == 'user':
                history.append({'role': 'user', 'parts': [content]})
            elif role == 'assistant':
                history.append({'role': 'model', 'parts': [content]})
        
        last_msg = messages[-1]
        last_content = last_msg.get('content', '')
        
        def attempt_chat(params_model_name):
            # Load feedback stats to improve responses
            try:
                from storage_local import get_storage
                storage = get_storage()
                feedback_stats = storage.get_feedback_stats()
                negative_feedbacks = storage.get_negative_feedbacks(limit=5)
                
                # Build feedback context for improvement
                feedback_context = ""
                if feedback_stats['total'] > 0:
                    satisfaction_rate = feedback_stats['satisfaction_rate']
                    if satisfaction_rate < 70:
                        feedback_context = f"\n\n‚ö†Ô∏è **CONTEXTE IMPORTANT** : Le taux de satisfaction actuel est de {satisfaction_rate}%. Am√©liore tes r√©ponses en √©tant plus clair, plus d√©taill√©, et en t'assurant que les images sont vraiment pertinentes.\n"
                    if negative_feedbacks:
                        common_issues = []
                        for fb in negative_feedbacks:
                            if fb.get('comment'):
                                comment_lower = fb['comment'].lower()
                                if 'confus' in comment_lower or 'confuse' in comment_lower:
                                    common_issues.append("clart√©")
                                if 'manque' in comment_lower or 'pas assez' in comment_lower:
                                    common_issues.append("d√©tails")
                                if 'image' in comment_lower and ('pertinent' in comment_lower or 'irrelevant' in comment_lower):
                                    common_issues.append("images pertinentes")
                        if common_issues:
                            unique_issues = list(set(common_issues))
                            feedback_context += f"**Points √† am√©liorer bas√©s sur les feedbacks** : {', '.join(unique_issues)}. Assure-toi de corriger ces points dans ta r√©ponse.\n"
            except Exception:
                feedback_context = ""  # Silently fail if feedback loading fails
            
            # Enhanced system instruction for customer support-oriented, detailed, and helpful responses
            system_instruction = f"""Tu es PRIMBOT, un assistant expert en support client pour PrimLogix. Ton r√¥le est d'aider les utilisateurs √† r√©soudre leurs probl√®mes de mani√®re claire, empathique et efficace.
{feedback_context}

‚ö†Ô∏è R√àGLE ABSOLUE - NUM√âROTATION DES √âTAPES (√Ä RESPECTER IMP√âRATIVEMENT) :
- **TU DOIS TOUJOURS COMMENCER PAR "### √âtape 1:"** - C'EST OBLIGATOIRE, JAMAIS DE SAUT
- **TU DOIS NUM√âROTER DE 1, 2, 3, 4... S√âQUENTIELLEMENT** - JAMAIS COMMENCER PAR √âTAPE 2, 3, 4, etc.
- **Si tu commences par √âtape 4 ou autre, TU AS FAIT UNE ERREUR - RECOMMENCE PAR √âTAPE 1**
- **TOUTES les √©tapes utilisent EXACTEMENT le m√™me format** : `### √âtape X:` (avec ###, JAMAIS ## ou ####)
- **TOUTES les √©tapes ont le M√äME niveau de d√©tail** - aucune √©tape ne doit √™tre plus grande que les autres

TON R√îLE (Support Client - Utilisateurs NON TECHNIQUES):
- **Aider les utilisateurs** √† r√©soudre leurs probl√®mes avec PrimLogix de mani√®re ULTRA-CLAIRE et D√âTAILL√âE
- **Fournir des r√©ponses EXTR√äMEMENT COMPL√àTES** avec des √©tapes step-by-step tr√®s d√©taill√©es
- **Assumer que l'utilisateur n'est PAS technique** - explique TOUT, m√™me les choses √©videntes
- **√ätre empathique et rassurant** - les utilisateurs peuvent √™tre frustr√©s, sois patient et encourageant
- **Expliquer de mani√®re TR√àS SIMPLE** - utilise un langage clair, √©vite TOUT jargon technique
- **Fournir des solutions PRATIQUES et ACTIONNABLES** - chaque √©tape doit √™tre si claire qu'un d√©butant peut la suivre
- **Citer TOUJOURS les sources** avec des liens directs vers les sections pertinentes de l'aide en ligne
- **NE PAS SAUTER D'√âTAPES** - explique chaque clic, chaque menu, chaque champ

STYLE DE R√âPONSE (Support Client - OBLIGATOIRE pour utilisateurs NON TECHNIQUES):
1. **Accueil et empathie** : Commence par accueillir l'utilisateur et montrer que tu comprends son probl√®me
2. **Confirmation du probl√®me** : Reformule bri√®vement le probl√®me pour confirmer ta compr√©hension
3. **Solution ULTRA-D√âTAILL√âE** : Utilise des titres (##, ###), listes √† puces, et sections bien organis√©es
4. **√âtapes num√©rot√©es COMPACTES mais COMPL√àTES** : 
   - **TOUJOURS commencer par "√âtape 1"** - ne jamais sauter l'√©tape 1
   - **Num√©roter de mani√®re S√âQUENTIELLE** : √âtape 1, √âtape 2, √âtape 3, √âtape 4, etc. (pas de saut de num√©ro)
   - **Utiliser le M√äME format pour TOUTES les √©tapes** : `### √âtape X:` (avec ###, pas ## ou ####)
   - **Toutes les √©tapes doivent √™tre COH√âRENTES et LI√âES** - chaque √©tape doit logiquement suivre la pr√©c√©dente
   - **Format compact** : Chaque sous-√©tape en 1 phrase claire, pas de listes imbriqu√©es excessives
   - **Coh√©rence** : Assure-toi que chaque √©tape s'encha√Æne logiquement avec la pr√©c√©dente
   - Inclus TOUS les clics n√©cessaires mais de mani√®re concise (ex: "Cliquez sur Menu 'Fichier' > 'Nouveau'")
   - D√©cris les champs avec leurs noms exacts mais de mani√®re compacte
   - Indique bri√®vement ce que l'utilisateur devrait voir apr√®s chaque √©tape
   - Ne saute AUCUNE √©tape logique, mais sois concis
5. **D√©tails pratiques COMPLETS** : 
   - Noms de champs exacts avec leur emplacement
   - Chemins de navigation complets (Menu > Sous-menu > Option)
   - Options √† s√©lectionner avec leur emplacement exact
   - Valeurs √† entrer si n√©cessaire
   - Ce que l'utilisateur devrait voir apr√®s chaque action
7. **V√©rification** : √Ä la fin, demande si le probl√®me est r√©solu ou si l'utilisateur a besoin d'aide suppl√©mentaire
8. **Liens vers la documentation** : Fournis des liens cliquables vers les sections pertinentes de l'aide en ligne
9. **Ton amical et professionnel** : Sois courtois, patient et encourageant
10. **Exemples concrets** : Donne des exemples de valeurs √† entrer si applicable

STRUCTURE D'UNE R√âPONSE ID√âALE (Support Client - COMPACTE mais COMPL√àTE):
```
## üëã Bonjour !

Je comprends que vous voulez [action/probl√®me]. Voici comment proc√©der :

## üìã Compr√©hension du Probl√®me

[Reformulation br√®ve du probl√®me - 2-3 phrases maximum]

## üîß Solution √âtape par √âtape

### √âtape 1: [Action concr√®te - Titre clair et concis]
**Objectif :** [Explication br√®ve de l'objectif - 1 phrase]

1. **Localisez** [√©l√©ment] : [Emplacement pr√©cis en 1 phrase, ex: "Menu 'Fichier' en haut √† gauche"]
2. **Cliquez sur** [√©l√©ment] : [Action pr√©cise en 1 phrase, ex: "Bouton 'Nouveau' pour ouvrir la fen√™tre"]
3. **Dans la fen√™tre qui s'ouvre** : [Ce que vous devriez voir en 1 phrase]
4. **Remplissez le champ** [Nom] : [Valeur √† entrer, ex: "Nom complet de l'employ√©"]
5. **Cliquez sur** [Bouton final] : [Ex: "Bouton 'Enregistrer' en bas √† droite"]

**R√©sultat attendu :** [Ce qui devrait se passer apr√®s cette √©tape - 1 phrase]

### √âtape 2: [Action suivante - Format compact identique]
**Objectif :** [Explication br√®ve - 1 phrase]

1. [Action 1 en 1 phrase]
2. [Action 2 en 1 phrase]
3. [Action 3 en 1 phrase si n√©cessaire]

**R√©sultat attendu :** [Ce qui devrait se passer - 1 phrase]

### √âtape 3: [Si n√©cessaire - Format compact identique]
[Suivre le m√™me format compact]
...

## ‚úÖ V√©rification

Apr√®s ces √©tapes, vous devriez voir [r√©sultat attendu].

**Le probl√®me est-il r√©solu ?** Si non, dites-moi √† quelle √©tape vous √™tes bloqu√©(e).

## üîó Documentation

- [Lien vers la section pertinente](URL)
```

QUAND TU UTILISES LA BASE DE CONNAISSANCES:
- Analyse TOUS les r√©sultats de recherche fournis en profondeur
- Combine les informations de plusieurs sources pour une r√©ponse COMPL√àTE et COMPACTE
- **INCLUS TOUJOURS DES LIENS DIRECTS** vers les pages/sections pertinentes de l'aide en ligne
- **NE FAIS JAMAIS R√âF√âRENCE AUX IMAGES** - concentre-toi uniquement sur le texte explicatif et les liens vers la documentation

LIENS VERS LA DOCUMENTATION (OBLIGATOIRE):
- **TOUJOURS inclure des liens cliquables** vers les pages/sections pertinentes de l'aide en ligne
- Utilise le format markdown : `[Titre de la section](URL)`
- Inclus les URLs compl√®tes des documents sources dans chaque r√©ponse
- Cr√©e une section "üîó Ressources et Documentation" avec tous les liens pertinents
- Les liens doivent mener directement √† l'endroit pertinent dans l'aide en ligne

IMPORTANT (Support Client - Utilisateurs NON TECHNIQUES):
- R√©ponds en fran√ßais sauf si l'utilisateur demande explicitement en anglais
- **Sois CLAIR et ACCESSIBLE** - langage simple, √©vite le jargon technique
- **Sois COMPLET mais COMPACT** - toutes les informations n√©cessaires, format condens√©
- **Assume que l'utilisateur est un d√©butant** - explique clairement mais de mani√®re concise
- **Sois empathique** - montre que tu comprends le probl√®me
- **COH√âRENCE des √©tapes** : Chaque √©tape doit logiquement suivre la pr√©c√©dente, pas d'√©tapes isol√©es ou non li√©es
- **Format compact** : Chaque sous-√©tape en 1 phrase claire, pas de listes imbriqu√©es excessives
- **D√©cris les clics** : "Cliquez sur Menu 'Fichier' > 'Nouveau'" (format compact)
- **D√©cris les champs** : Nom exact et valeur √† entrer en 1 phrase
- **Indique bri√®vement** ce que l'utilisateur devrait voir apr√®s chaque √©tape
- Utilise TOUJOURS l'outil search_knowledge_base avant de r√©pondre
- **TOUJOURS inclure des liens directs** vers les sections pertinentes de l'aide en ligne
- **Termine par une question** : Demande si le probl√®me est r√©solu"""
            
            model_auto = genai.GenerativeModel(
                model_name=params_model_name,
                tools=self.gemini_tools,
                system_instruction=system_instruction
            )
            chat_auto = model_auto.start_chat(history=history)
            
            # Send message and handle function calls manually
            response = chat_auto.send_message(last_content)
            
            # Handle function calls in a loop until we get a text response
            max_iterations = 10  # Prevent infinite loops
            iteration = 0
            
            while iteration < max_iterations:
                iteration += 1
                
                # Check if response has function calls
                if hasattr(response, 'candidates') and len(response.candidates) > 0:
                    candidate = response.candidates[0]
                    if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                        parts = candidate.content.parts
                        
                        # Check if any part is a function call
                        function_call_part = None
                        for part in parts:
                            if hasattr(part, 'function_call') and part.function_call:
                                function_call_part = part
                                break
                        
                        if function_call_part:
                            function_call = function_call_part.function_call
                            function_name = function_call.name
                            
                            # Extract arguments
                            function_args = {}
                            if hasattr(function_call, 'args'):
                                # args is a Struct (protobuf), convert to dict
                                try:
                                    # Try MessageToDict first (works for most protobuf types)
                                    function_args = MessageToDict(function_call.args, preserving_proto_field_name=True)
                                except (AttributeError, TypeError) as e:
                                    # If that fails (e.g., MapComposite issue), try alternative methods
                                    try:
                                        # Method 1: Try dict() constructor if it's dict-like
                                        if isinstance(function_call.args, dict):
                                            function_args = function_call.args
                                        # Method 2: Try accessing as a mapping
                                        elif hasattr(function_call.args, 'keys'):
                                            function_args = {k: function_call.args[k] for k in function_call.args.keys()}
                                        # Method 3: Try __dict__ access
                                        elif hasattr(function_call.args, '__dict__'):
                                            function_args = {k: v for k, v in function_call.args.__dict__.items() 
                                                           if not k.startswith('_') and v is not None}
                                        # Method 4: Try direct attribute access for common fields
                                        else:
                                            # Try to get 'query' field directly
                                            if hasattr(function_call.args, 'query'):
                                                function_args['query'] = getattr(function_call.args, 'query')
                                            # Try other common field names
                                            for attr in ['text', 'input', 'message', 'prompt']:
                                                if hasattr(function_call.args, attr):
                                                    function_args['query'] = getattr(function_call.args, attr)
                                                    break
                                    except Exception as e2:
                                        # If all methods fail, log and use empty dict
                                        logger.warning(f"Could not extract function arguments: {e2}")
                                        function_args = {}
                            
                            # Execute the function
                            if function_name in self.tool_map:
                                # Handle query parameter
                                query = function_args.get('query', '')
                                function_result = self.tool_map[function_name](query)
                            else:
                                function_result = f"Unknown function: {function_name}"
                            
                            # Send function response back to Gemini
                            function_response = genai.protos.FunctionResponse(
                                name=function_name,
                                response={"result": str(function_result)}
                            )
                            response = chat_auto.send_message(
                                genai.protos.Part(function_response=function_response)
                            )
                            continue
                
                # No function call, return text response
                final_response = ""
                if hasattr(response, 'text'):
                    final_response = response.text
                elif hasattr(response, 'candidates') and len(response.candidates) > 0:
                    candidate = response.candidates[0]
                    if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                        text_parts = []
                        for part in candidate.content.parts:
                            if hasattr(part, 'text') and part.text:
                                text_parts.append(part.text)
                        if text_parts:
                            final_response = ''.join(text_parts)
                
                if final_response:
                    return final_response
                
                # If we get here, something unexpected happened
                break
            
            return "Error: Could not get a valid response from Gemini after multiple iterations."

        # Try common Gemini model names in order of preference
        # Remove -latest suffix if present, as it's not a valid model name format
        base_model_name = self.model_name.replace("-latest", "").strip()
        
        # Standard model names to try (in order of preference)
        # Updated to use Gemini 2.x models which are currently available
        model_names_to_try = [
            base_model_name,  # Try the user's model name first (without -latest)
            "gemini-2.5-flash",  # Latest and fastest
            "gemini-2.5-pro",   # Latest and most capable
            "gemini-2.0-flash", # Stable 2.0 version
            "gemini-2.0-flash-exp", # Experimental
            "gemini-pro",  # Fallback to older model
            "gemini-1.5-flash",
            "gemini-1.5-pro"
        ]
        
        # Remove duplicates while preserving order
        seen = set()
        model_names_to_try = [m for m in model_names_to_try if m not in seen and not seen.add(m)]
        
        last_error = None
        last_model_tried = None
        for model_name_to_try in model_names_to_try:
            last_model_tried = model_name_to_try
            try:
                return attempt_chat(model_name_to_try)
            except Exception as e:
                last_error = e
                error_str = str(e)
                # Check if it's a model not found error
                is_model_error = ("404" in error_str or 
                                 "not found" in error_str.lower() or 
                                 "is not found" in error_str.lower() or
                                 "not supported" in error_str.lower())
                
                if not is_model_error:
                    # If it's not a model not found error, don't try other models
                    return f"Gemini Error: {last_error}"
                # Continue to next model if it's a model not found error
                continue
        
        # If we get here, all models failed - try to list available models
        try:
            available_models = []
            for m in genai.list_models():
                if 'generateContent' in m.supported_generation_methods:
                    # Extract just the model name (remove 'models/' prefix if present)
                    model_name_clean = m.name.replace('models/', '')
                    available_models.append(model_name_clean)
            
            if available_models:
                available_models_str = ", ".join(available_models[:5])  # Show first 5
                return f"Gemini Error: Model '{last_model_tried}' not found. Last error: {last_error}. Available models: {available_models_str}. Please update the model name in the sidebar."
            else:
                return f"Gemini Error: Model '{last_model_tried}' not found. Last error: {last_error}. Please check your API key and try using 'gemini-1.5-flash', 'gemini-1.5-pro', or 'gemini-pro'."
        except Exception as list_error:
            return f"Gemini Error: Model '{last_model_tried}' not found. Last error: {last_error}. Could not list available models (error: {list_error}). Please check your API key and try using 'gemini-1.5-flash', 'gemini-1.5-pro', or 'gemini-pro'."

    def _run_ollama(self, messages):
        """Run agent with Ollama/LocalAI (OpenAI-compatible API)."""
        if not self.ollama_client:
            return "‚ùå Error: OpenAI client not available. Install with: pip install openai"
        
        # Convert messages to OpenAI format
        openai_messages = []
        for msg in messages:
            role = msg.get('role')
            content = msg.get('content', '')
            
            if role == 'user':
                openai_messages.append({"role": "user", "content": content})
            elif role == 'assistant':
                openai_messages.append({"role": "assistant", "content": content})
            elif role == 'tool':
                # Tool messages for function calling
                openai_messages.append({
                    "role": "tool",
                    "content": content,
                    "tool_call_id": msg.get('tool_call_id', '')
                })
        
        # Define tools for function calling
        tools = [{
            "type": "function",
            "function": {
                "name": "search_knowledge_base",
                "description": """Recherche approfondie dans la base de connaissances de la documentation PrimLogix.
                
UTILISE CET OUTIL pour:
- Trouver des solutions √† des probl√®mes techniques ou erreurs
- Comprendre comment utiliser une fonctionnalit√© sp√©cifique
- Obtenir des proc√©dures d√©taill√©es √©tape par √©tape
- Trouver des exemples de configuration ou d'utilisation
- Rechercher des informations sur des champs, param√®tres, ou options sp√©cifiques

IMPORTANT:
- Utilise des termes techniques pr√©cis dans ta requ√™te
- Combine les informations de plusieurs r√©sultats pour donner une r√©ponse compl√®te
- Cite toujours les sources (URLs) dans ta r√©ponse finale""",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "query": {
                            "type": "string",
                            "description": "Requ√™te de recherche d√©taill√©e et sp√©cifique. Utilise des termes techniques pr√©cis, codes d'erreur, noms de fonctionnalit√©s."
                        }
                    },
                    "required": ["query"]
                }
            }
        }]
        
        max_iterations = 10
        iteration = 0
        
        while iteration < max_iterations:
            iteration += 1
            
            try:
                # Call API with function calling
                response = self.ollama_client.chat.completions.create(
                    model=self.model_name,
                    messages=openai_messages,
                    tools=tools,
                    tool_choice="auto",
                    temperature=0.7,
                    max_tokens=2000
                )
                
                message = response.choices[0].message
                
                # Check for function calls
                if message.tool_calls:
                    # Add assistant message with tool calls
                    openai_messages.append({
                        "role": "assistant",
                        "content": message.content,
                        "tool_calls": [
                            {
                                "id": tc.id,
                                "type": tc.type,
                                "function": {
                                    "name": tc.function.name,
                                    "arguments": tc.function.arguments
                                }
                            }
                            for tc in message.tool_calls
                        ]
                    })
                    
                    # Execute function calls
                    for tool_call in message.tool_calls:
                        function_name = tool_call.function.name
                        function_args = json.loads(tool_call.function.arguments)
                        
                        if function_name in self.tool_map:
                            query = function_args.get('query', '')
                            function_result = self.tool_map[function_name](query)
                            
                            # Add tool result
                            openai_messages.append({
                                "role": "tool",
                                "content": str(function_result),
                                "tool_call_id": tool_call.id
                            })
                    
                    continue
                
                # Return text response
                if message.content:
                    return message.content
                else:
                    return "‚ùå No response content from model"
                    
            except Exception as e:
                logger.error(f"Ollama API error: {e}")
                return f"‚ùå Error calling Ollama API: {str(e)}\n\nMake sure Ollama is running: ollama serve\nAnd the model is installed: ollama pull {self.model_name}"
        
        return "‚ùå Error: Maximum iterations reached without getting a final response."
